---
output: 
  pdf_document:
    citation_package: natbib
    latex_engine: pdflatex
title: "HonestDiD: Vignette"
author: "Ashesh Rambachan"
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
bibliography: bibliography.bib
fontsize: 11pt
vignette: >
  %\VignetteIndexEntry{dfadjust}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE, cache = FALSE}
library('knitr')
knitr::opts_knit$set(self.contained = FALSE)
knitr::opts_chunk$set(tidy = TRUE, collapse=TRUE, comment = "#>",
                      tidy.opts=list(blank=FALSE, width.cutoff=60))
```

# Description

The `HonestDiD` package implements the methods developed in @RambachanRoth for performing inference in difference-in-differences and event-study designs that is robust to violations of the parallel trends assumption. See @RambachanRoth for methodological details.

We illustrate how the package can be used by replicating Figure 9 in @RambachanRoth, which applies these methods to @LovenheimWillen's results on the effect of public sector bargaining laws on long-run female labor market outcomes.The estimated event study coefficients and variance-covariance matrix for the baseline female estimates on employment in Specification (1) of @LovenheimWillen are included in the package as `LWdata_EventStudy`. The underlying raw data for this event study is also included in the package.

# Installation

To install the package, use the function `install_github()` from the `devtools` package:
```{r, eval = FALSE}
install.packages("devtools") # if devtools package not installed
devtools::install_github("asheshrambachan/HonestDiD")
```

After installation, load in the package using `library('HonestDiD')`.

```{r, include=FALSE}
library('HonestDiD')
```
 
# Preliminaries

The included dataframe `LWdata_EventStudy` contains the estimated event study coefficients and estimated variance-covariance matrix for the baseline estimates for female employment based on Specification (1) of @LovenheimWillen. As discussed in Section 10 of @RambachanRoth, the authors use the American Community Survey to study the impact of public-sector duty-to-bargain (DTB) laws, which strengthen teachers' unions, on adult labor market outcomes. Exploiting the differential timing of the passage of DTB laws across states, the authors estimate the event study specification
$$
Y_{sct} = \sum_{r=-11}^{21} D_{scr} \beta_{r} + X_{sct}^\prime \gamma + \lambda_{ct} + \phi_s + \epsilon_{sct},
$$
where $Y_{sct}$ is the average employment for a cohort of students born in state $s$ in cohort $c$ in ACS calendar year $t$ and $D_{scr}$ is an indicator for whether state $s$ passed a DTB law $r$ years before cohort $c$ turned age 18. `LWdata_EventStudy` contains the estimated event study coefficients $\hat \beta_{r}$, the associated variance-covariance matrix of these estimates and some additional information about the event study specification. The next code snippet loads the data.

```{r}
data('LWdata_EventStudy', package = "HonestDiD")

# Number of pre-periods
numPrePeriods = length(LWdata_EventStudy$prePeriodIndices)
numPostPeriods = length(LWdata_EventStudy$postPeriodIndices)
```

We provide additional details on how the results in `LWdata_EventStudy` are constructed in the section titled ``Details on replicating @LovenheimWillen'' below.

# Conducting Sensitivity Analyses

In this section, we show how to use the package `HonestDiD` to conduct a formal sensitivity analysis using the estimated event study from @LovenheimWillen.

## Background: choices of $\Delta$

Following @RambachanRoth, the parameter of interest is $\theta = l'\tau_{post}$, where $\tau_{post}$ is the vector of dynamic causal effects of interest in the post-periods and $l$ is a vector specified by the user. For instance, if the user is interested in the effect in the first period after treatment, then $l$ should be set to the basis vector with a 1 in the 1st position and zeros elsewhere. To construct confidence sets for $\theta$ that are robust to violations of the parallel trends assumption, the user must specify the set $\Delta$, which describes the set of possible violations of the parallel trends assumption that the user is willing to consider. 

The `HonestDiD` package currently allows for several choices of $\Delta$. First, a reasonable baseline in many cases is $\Delta = \Delta^{SD}(M)$, which requires that the underlying trend not deviate ``too much'' from linearity. Specifically, it imposes that the change in the slope of the underlying trend (i.e. the second difference in $\delta$) be no more than $M$ between consecutive periods. Formally, this set is defined as 
$$ 
  \Delta^{SD}(M) = \{\delta : | (\delta_t - \delta_{t-1}) - (\delta_{t-1} - \delta_{t-2}) | \leq M\}.
$$
For the choice $M = 0$, this choice of $\Delta$ limits the violation of parallel trends to be linear, while $M>0$ relaxes the assumption of exact linearity. See Section 2.3 of @RambachanRoth for further discussion. 

The user may additionally restrict the sign of the bias in the post-period. This may be reasonable, for instance, in cases where there is a simulataneous policy change which we think affects the outcome of interest in a particular direction. We will refer to restrictions that combine $\Delta^{SD}$ with a restriction on the post-period bias by $\Delta = \Delta^{SDB}(M)$. For example, $\Delta^{SDPB}(M)$ additionally imposes that the violation of parallel trends must be positive in the post-periods, $\delta_t \geq 0$ for $t \geq 0$. Likewise, $\Delta^{SDNB}(M)$ additionally imposes that the violation of parallel trends must be negative in the post-periods $\delta_t \leq 0$ for $t \geq 0$. 

Finally, the researcher may specify additional shape restrictions that specify that the violation of parallel trends must be monotonically increasing or decreasing. Such restrictions may be reasonable in cases where the researcher is concerned about secular trends that would have continued absent treatment. We will refer to restrictions that combine $\Delta^{SD}$ with monotonicity restrictions by $\Delta = \Delta^{SDM}(M)$. In the case where the violation of parallel trends must be increasing, denoted by $\Delta^{SDI}(M)$, this additionally restricts $\delta_{t} \geq \delta_{t-1}$ for all $t$. In the case where the violation of parallel trends must be decreasing, denoted by $\Delta^{SDD}(M)$, this additionally restricts $\delta_{t} \leq \delta_{t-1}$ for all $t$. 

## Constructing a sensitivity plot 

We next show to use the package `HonestDiD` to conduct a formal sensitivity analysis. We recommend that the user creates a sensitivity plot that shows how the robust confidence sets vary under different assumptions about $\Delta$ (e.g., letting $M$ vary or adding sign/shape restrictions).

The function `constructRobustCI` provides a wrapper function to conduct sensitivity analysis for the choices of $\Delta$ discussed earlier. This function takes as inputs the estimated event study coefficients, the estimated variance-covariance matrix of the estimates along with the user's choice of $\Delta$ and chosen method for constructing robust confidence intervals. It returns the upper and lower bounds of the robust confidence sets for a vector of choices of $M$ as a dataframe. The researcher may specify that $\Delta$ equals $\Delta^{SD}(M)$, $\Delta^{SDB}(M)$ or $\Delta^{SDM}(M)$. In the latter two cases, the user additionally specifies the sign/direction of the bias/monotonicity. 

As a default, the vector of $M$ values is chosen to be a sequence of ten values from $0$ to 50\% of the standard error of the first post-period coefficient $\hat \beta_1$ by default. If the user leaves the desired method as `NULL`, the function automatically selects the robust confidence interval based upon the recommendations in Section 9 of @RambachanRoth. (If $\Delta = \Delta^{SD}(M)$, the FLCI is used. If $\Delta = \Delta^{SDB}(M)$ or $\Delta = \Delta^{SDM}(M)$, the conditional FLCI hybrid confidence set is used.) As a default, the function sets the parameter of interest to be the first post-period causal effect, $\theta = \tau_1$. The user may directly specify the parameter of interest by setting the input `l_vec`.

In the next code snippet, we conduct the sensitivity analysis plotted in Figure 9a) in @RambachanRoth, which shows a sensitivity analysis using $\Delta = \Delta^{SD}(M)$ for the effect on female employment after 15 years of exposure to a duty-to-bargain law, $\theta = \tau_{15}$.

```{r warning = FALSE}

#Create l_vec corresponding with 15 years of exposure
  #Reference is -2 years of exposure, so want effect 17 pds later 
l_vec = basisVector(15 - (-2), numPostPeriods)

# Construct robust confidence intervals for Delta^{SD}(M) for 15 years of exposure
DeltaSD_RobustResults = createSensitivityResults(betahat = LWdata_EventStudy$betahat, 
                                          sigma = LWdata_EventStudy$sigma, 
                                          Delta = "DeltaSD",
                                          numPrePeriods = numPrePeriods, 
                                          numPostPeriods = numPostPeriods,
                                          l_vec = l_vec, 
                                          Mvec = seq(from = 0, to = 0.04, by = 0.005))
head(DeltaSD_RobustResults)
```

The function `createSensitivityPlot` can then be used to construct a sensitivity plot presenting these results. `createSensitivityPlot` takes two key inputs. The first input is the dataframe that is produced by `constructRobustCI`, which contains the robust confidence intervals. The second input is a dataframe that contains the OLS confidence set for the parameter of interest. This dataframe can be constructed using the function `constructOLSCI`. In the next code snippet, we show how these functions can be used to replicate Figure 9a) in @RambachanRoth. 

```{r warning = FALSE}
# Construct dataframe with OLS confidence interval for theta
OriginalResults = constructOriginalCS(betahat = LWdata_EventStudy$betahat, 
                                          sigma = LWdata_EventStudy$sigma, 
                                          Delta = "DeltaSD",
                                          numPrePeriods = numPrePeriods, 
                                          numPostPeriods = numPostPeriods,
                                          l_vec = l_vec )

# Construct sensitivity plot
DeltaSD_SensitivityPlot = createSensitivityPlot(robustResults = DeltaSD_RobustResults, 
                                                OLSresults = OLSresults)
DeltaSD_SensitivityPlot
```

In the next code snippet, we replicate the sensitivity analysis in Figure 9b) of @RambachanRoth. The exercise is similar to that shown above, except we now impose that any violations of parallel trends be (weakly) decreasing ($\Delta = \Delta^{SDD}(M)$). This incorporates the intuition from @LovenheimWillen that the pre-trends for women are likely due to secular trends in female labor supply that would have continued absent treatment.  

```{r warning = FALSE}
# Construct robust confidence intervals for Delta^{SD}(M)
DeltaSDD_RobustResults = createSensitivityResults(betahat = LWdata_EventStudy$betahat, 
                                          sigma = LWdata_EventStudy$sigma, 
                                          Delta = "DeltaSDM", 
                                          monotonicityDirection = "decreasing",
                                          numPrePeriods = numPrePeriods, 
                                          numPostPeriods = numPostPeriods,
                                          l_vec = l_vec,
                                          Mvec = seq(from = 0, to = 0.04, by = 0.005))

# Construct sensitivity plot 
DeltaSDD_SensitivityPlot = createSensitivityPlot(robustResults = DeltaSDD_RobustResults, 
                                                OLSresults = OLSresults)
DeltaSDD_SensitivityPlot
```

## Benchmarking $M$

The sensitivity plots discussed above show how our conclusions change as we allow for larger degrees of possible non-linearity in the violations of parallel trends, parameterized by $M$. @RambachanRoth discuss multiple ways for benchmarking $M$ in applied settings.

One approach for benchmarking $M$ is to use context-specific knowledge about the magnitudes of potential confounds. For instance, in the context of @LovenheimWillen, one concern is differential changes in education quality that would have occurred even absent the passage of DTB laws. Section 10 of @RambachanRoth calibrates $M$ using estimates of the effect of teacher quality on adult employment from @CFR2014. In this calibration, a value of $M = 0.01$ corresponds with a change in slope of the diffferential trend corresponding with a change in teacher quality of $0.025$ standard deviations. 

In some cases, it may also be useful to benchmark $M$ -- which bounds the change in slope of the differential trend between consecutive periods -- using estimates of the largest change in slope in the pre-period. We provide the functions `DeltaSD_lowerBound_M` and `DeltaSD_upperBound_M`, which create one-sided confidence intervals for the largest change in slope in the pre-period. Values of $M$ below the values computed by `DeltaSD_lowerBound_M` are rejected by the data (at the given significance level), and thus should be viewed with caution. On the other hand, we stress that data from the pre-period cannot, on its own, place an upper bound on the possible degree of non-linearity under the counterfactual in the post-periods. However, in some cases it may be useful to benchmark the assumed maximal degree of non-linearity $M$ in terms of the largest change in slope in the pre-period. These functions can also be used analogously to benchmark $M$ using event-studies for placebo groups. See Section 9 of @RambachanRoth and the R documentaiton for additional details.

Both functions require the user to specify the vector of estimated event study coefficients, the variance covariance matrix, the number of pre-periods and the desired size of the one-sided confidence intervals. They can be used as follows:

```{r}
lowerBound_M = DeltaSD_lowerBound_M(betahat = LWdata_EventStudy$betahat, sigma = LWdata_EventStudy$sigma, 
                                    numPrePeriods = numPrePeriods)
upperBound_M = DeltaSD_upperBound_M(betahat = LWdata_EventStudy$betahat, sigma = LWdata_EventStudy$sigma, 
                                    numPrePeriods = numPrePeriods)
```


# Details on replicating @LovenheimWillen

We now provide additional details on the construction of the event-study results from @LovenheimWillen used in the example above. This event study specification can be fully replicated using a dataframe provided in the package `HonestDiD`. Within the subdirectory `inst/extdata`, the package provides the stata dataset `LWdata_RawData.dta`. This contains the estimation sample for females that is used to estimate the above event study specification. This dataset is provided in the replication files for @LovenheimWillen and can be found in the subdirectory of the replication files, `Data files/Estimation samples/Estimation_FEMALE.dta`. The following code snippet shows how to reproduce the event study above using the provided data

```{r eval = FALSE}
# Load in LWdata_RawData.dta
LWdata_RawData = haven::read_dta(system.file("extdata", "LWdata_RawData.dta", 
                                             package = "HonestDiD"))

# Estimate event study using lfe package
EmpFemale.EventStudy = lfe::felm(emp ~ rtESV13 + rtESV14 + rtESV15 +
                                          rtESV16 + rtESV17 + rtESV18 +
                                          rtESV19 + rtESV110 + rtESV111 + # End Pre-periods 
                                          rtESV113 + rtESV114 + rtESV115 + 
                                          rtESV116 + rtESV117 + rtESV118 + 
                                          rtESV119 + rtESV120 + rtESV121 + 
                                          rtESV122 + rtESV123 + rtESV124 + 
                                          rtESV125 + rtESV126 + rtESV127 + 
                                          rtESV128 + rtESV129 + rtESV130 + 
                                          rtESV131 + rtESV132 + rtESV133 + 
                                          rtESV134 + rtESV135 + # End post-periods
                                          yearsfcor + yearsflr + aveitc + fscontrol + 
                                          asian + black + hispanic + other | 
                                          factor(PUS_SURVEY_YEAR)*factor(BIRTHYEAR) + 
                                          factor(PUS_SURVEY_YEAR) + factor(BIRTHSTATE) | 
                                          0 | BIRTHSTATE,
                                        data = LWdata_RawData,
                                        weights = LWdata_RawData$nobs)

# Extract coefficients of regression associated with event study coefficients
coefIndex = which(grepl(x = dimnames(EmpFemale.EventStudy$coefficients)[[1]],
                        pattern = "rtESV"))
betahat = EmpFemale.EventStudy$beta[coefIndex, ]

# Extract estimated variance-covariance matrix of event study coefficients
sigma = EmpFemale.EventStudy$clustervcv[coefIndex, coefIndex]

#Rescale by 100 so that results will be in units of percentage points
betahat = 100 * betahat
sigma = 100^2 * sigma

# Construct vector of event times and the scalar reference period
timeVec = c(seq(from = -11, to = -3, by = 1), seq(from = -1, to = 21, by = 1))
referencePeriod = -2
postPeriodIndices = which(timeVec > -2)
prePeriodIndices = which(timeVec < -2)

# Construct standard errors associated with event study coefficients
stdErrors = summary(EmpFemale.EventStudy)$coefficients[coefIndex,2]

# Create list containing objects produced by the event study
LWdata_EventStudy = list(
  betahat = betahat, 
  sigma = sigma, 
  timeVec = timeVec, 
  referencePeriod = referencePeriod, 
  prePeriodIndices = prePeriodIndices, 
  postPeriodIndices = postPeriodIndices, 
  stdErrors = stdErrors
)
```

\newpage
# References
